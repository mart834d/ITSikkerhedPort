#!/usr/bin/env bash
# eksemplar:
#   ./GetAllIpsFromWeblog.txt access.log
#   ./GetAllIpsFromWeblog.txt /var/log/nginx/*.log.gz
#   ./GetAllIpsFromWeblog.txt -r /var/log/nginx -t 20 -c
#
# Options:
#   -r DIR         Scan directory recursively for *.log and *.log.gz files
#   -o FILE        Write output to FILE (otherwise stdout)
#   -c             Show counts (frequency) sorted desc instead of just unique list
#   -t N           Show top N results (only with -c); default = all
#   -j             Output JSON (only with -c; produces [{"ip":"x","count":n},...])
#   -h             Show this help

set -euo pipefail

show_help() {
  sed -n '1,160p' "$0" | sed -n '1,200p'
  cat <<'EOF'

Notes:
- IPv4 validation ensures octets are 0-255.
- IPv6 matching is permissive (covers most normal addresses) but may be permissive in edge cases.
- Works with plain text and gzipped logs (.gz).
EOF
  exit 0
}

# defaults
RECURSIVE=false
OUTFILE=""
COUNT_MODE=false
TOP_N=0
JSON_OUT=false

while getopts ":r:o:ct:jh" opt; do
  case $opt in
    r) RECURSIVE=true; DIR_TO_SCAN="$OPTARG" ;;
    o) OUTFILE="$OPTARG" ;;
    c) COUNT_MODE=true ;;
    t) TOP_N="$OPTARG" ;;
    j) JSON_OUT=true ;;
    h) show_help ;;
    *) show_help ;;
  esac
done
shift $((OPTIND-1))

# gather input files (either args or recursive dir)
FILES=()
if [ "${RECURSIVE}" = true ]; then
  if [ -z "${DIR_TO_SCAN:-}" ]; then
    echo "Error: -r requires a directory argument" >&2
    exit 2
  fi
  # find .log and .log.gz files
  while IFS= read -r -d $'\0' f; do FILES+=("$f"); done < <(find "$DIR_TO_SCAN" -type f \( -name "*.log" -o -name "*.log.*" -o -name "*.log.gz" -o -name "*.gz" \) -print0)
else
  if [ "$#" -ge 1 ]; then
    # accept globbing
    for a in "$@"; do
      FILES+=("$a")
    done
  else
    # read stdin
    FILES+=("-") 
  fi
fi

# If no files found, exit
if [ "${#FILES[@]}" -eq 0 ]; then
  echo "No log files found." >&2
  exit 0
fi

# Function to cat file (handles "-" for stdin and gz)
cat_log() {
  local f="$1"
  if [ "$f" = "-" ]; then
    cat -
  elif [[ "$f" == *.gz || "$f" == *.tgz ]]; then
    gzip -dc -- "$f" 2>/dev/null || zcat -- "$f" 2>/dev/null
  else
    cat -- "$f"
  fi
}

# Regexes:
# IPv4: naive match of 4 octets, then validated by awk (0-255)
IPV4_RE='([0-9]{1,3}\.){3}[0-9]{1,3}'
# IPv6: permissive pattern (contains hex and colons). This will catch typical IPv6 addresses.
IPV6_RE='([0-9a-fA-F]{1,4}:){2,7}[0-9a-fA-F]{1,4}'

# Pipeline:
# 1) stream logs
# 2) grep -oE to extract candidate IP-like tokens (both IPv4 and IPv6)
# 3) awk: validate IPv4 ranges, normalize; pass-through IPv6
# 4) sort/uniq or count as requested

# Build a temporary fifo to stream files (avoids command-line length limits)
tmpf=$(mktemp -u)
mkfifo "$tmpf"
# background process to write all files into fifo
{
  for f in "${FILES[@]}"; do
    # ignore non-existent globs
    if [ "$f" != "-" ] && [ ! -e "$f" ]; then
      # try to expand glob patterns
      matches=( $f )
      if [ "${#matches[@]}" -eq 0 ]; then
        continue
      fi
    fi
    cat_log "$f"
  done
  rm -f "$tmpf"
} > "$tmpf" &

# Now read from fifo and process
PROCESS_CMD="cat \"$tmpf\" | grep -oE --binary-files=text \"$IPV4_RE|$IPV6_RE\" | awk '
function ipv4_valid(ip) {
  n = split(ip, a, \".\")
  if (n != 4) return 0
  for (i = 1; i <= 4; i++) {
    if (a[i] !~ /^[0-9]+$/) return 0
    if (a[i] < 0 || a[i] > 255) return 0
  }
  return 1
}
{
  if (index($0, \":\") > 0) {
    # probable IPv6 - print as-is (could be further normalized if needed)
    print $0
  } else {
    # IPv4 candidate - validate
    if (ipv4_valid($0)) print $0
  }
}'"

# Evaluate the pipeline and then final aggregation
if [ "$COUNT_MODE" = true ]; then
  # produce counts
  # sort, uniq -c, sort -nr
  eval "$PROCESS_CMD" | sort -u | true  # ensure pipeline starts even if empty (we will re-run without -u)
  # The above line ensures fifo reader started; now run actual counting pipeline:
  RESULTS=$(eval "$PROCESS_CMD" | sort | uniq -c | sort -rn)
  if [ -n "$TOP_N" ] && [ "$TOP_N" -gt 0 ]; then
    RESULTS=$(printf "%s\n" "$RESULTS" | head -n "$TOP_N")
  fi

  if [ "$JSON_OUT" = true ]; then
    # produce JSON array [{"ip":"x","count":n},...]
    # convert RESULTS lines like "  42 1.2.3.4"
    json=$(printf "%s\n" "$RESULTS" | awk '{
        count=$1; $1=""; sub(/^ +/,"");
        ip=$0;
        gsub(/"/, "\\\"", ip);
        printf "{\"ip\":\"%s\",\"count\":%d}\n", ip, count
      }' | awk 'BEGIN{first=1; printf "["} { if(!first) printf ","; first=0; printf "%s", $0 } END{ printf "]" }')
    if [ -n "$OUTFILE" ]; then
      printf "%s\n" "$json" > "$OUTFILE"
    else
      printf "%s\n" "$json"
    fi
  else
    if [ -n "$OUTFILE" ]; then
      printf "%s\n" "$RESULTS" > "$OUTFILE"
    else
      printf "%s\n" "$RESULTS"
    fi
  fi

else
  # unique list
  if [ -n "$OUTFILE" ]; then
    eval "$PROCESS_CMD" | sort -u > "$OUTFILE"
  else
    eval "$PROCESS_CMD" | sort -u
  fi
fi

# cleanup is handled by background block removing fifo
exit 0